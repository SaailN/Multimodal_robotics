## Code for using LLM to plan the tasks.

This repository contains the central framework, which using the available Robot APIs (pick/place) and utility APIs (object detection) prompts an LLM to generate code for executing a given task.

### Getting Started

1. Go to Gemini AI console and generate your free API key. 
2. Make a `.env` file and put the key in this format - 
`API_KEY=<YOUR_API_KEY>`
3. Install the requirements. `pip install -r requirements.txt`
4. Install Huggingface Transformers and PyTorch (depending on your operating system). Also install PyAudio and openao-whisper if you want to use speech input for the task.
5. Run main.py

## More Information

- _main.py_ contains the driver code. It uses the configuration defined in _constants.py_, to generate sentences defined in _prompts.py_, and places them in appropriate slots as defined in _templates.py_. The prompt also contains a sample task, which is also defined in _prompts.py_.
- The prompt generated by _main.py_ can be seen in _prompt.txt_. This prompt is either sent to Gemini, or Llama3. For Gemini, the generated prompt is enough. For Llama3, RAG is used (via the input-output pairs in data/) to generate the correct code output.
- The code generated by the LLM can be seen in _output.py_. The code calls functions defined in _robots.py_ and _utils.py_. _robots.py_ contains the robot APIs, and _utils.py_ contains the utility APIs. The object detector in _utils.py_ is either Google's Owlv2, or PaliGemma. Depending on the performance and speed requirements, one can be chosen over the other.
- The generated code is then executed by sending relevant API requests to the ROS server.
