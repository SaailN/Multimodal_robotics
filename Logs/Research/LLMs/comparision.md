# COMPARISION BETWEEN DIFFERENT LLMs
|FACTOR|*BERT* |*GPT 2*|*GPT 3*|*GPT 3.5* |*GPT 4*|*LLaMA*|*GEMINI*
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|**Developer**|Google|OpenAI|OpenAI|OpenAI|OpenAI|MetaAI|Google
|**Accuracy**|HIGH (limited in text-generation)|Good|Better than GPT 2|Better than GPT 3|Best among GPT |HIGH |VERY GOOD
|**Training Objective**| (MLM) and (NSP)|NWP(Next Word Predictiion)|NWP|NWP|NWP|NWP|NWP with advanced learning aim
|**Architecture**|transformer encoder , bi - directional masked models , pre-training and fine tuning |transformer decoder , uni-directional causal models ,only massive pre-training  | same as GPT 2|improved version of 3|improved version of 3.5|Transformer based, uses BPE model|Transformer-decoder based(with  techniques AlphaGo, AlphaZero) MultiModal
|**Model size**|BERT Base: 110M params BERT Large: 340M params|1.7B param|175B param|175B params|not disclosed|65B params|3.25B param
|**Training data** |Wikipedia and BooksCorpus|WebText dataset|Common Crawl, WebText,Wikipedia|Similar to GPT 3|not disclosed| CommonCrawl,GitHub,Wikipedia, StackExchange, ArXiv|not disclosed
|**Availability**|open source|closed source|given access to its API|closed source|closed source|open source|closed source



